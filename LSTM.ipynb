{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/train.csv', index_col='id')\n",
    "data_test = pd.read_csv('data/test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = data_train['text'].tolist() + data_test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 2000 # number of words in dictionary\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_train_docs = tokenizer.texts_to_sequences(list(data_train['text']))\n",
    "encoded_test_docs = tokenizer.texts_to_sequences(list(data_test['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding additional columns to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train['tokens'] = pd.Series(encoded_train_docs, index=data_train.index)\n",
    "data_test['tokens'] = pd.Series(encoded_test_docs, index=data_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train['tokens_len'] = data_train['tokens'].apply(len)\n",
    "data_test['tokens_len'] = data_test['tokens'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers. Dirty for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 70 # see EDA to exploration of this legth\n",
    "data_train = data_train[data_train['tokens_len'] <= max_length]\n",
    "data_test = data_test[data_test['tokens_len'] <= max_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unifying all sequences to one length and preparing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "padded_docs = pad_sequences(data_train['tokens'], maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "labels = encoder.fit_transform(data_train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19411, 70) (19411, 3)\n"
     ]
    }
   ],
   "source": [
    "print(padded_docs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling and spliting dataset to test/validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_dataset = np.concatenate((labels, padded_docs), axis=1)\n",
    "np.random.shuffle(full_dataset)\n",
    "\n",
    "Y = full_dataset[:,:3]\n",
    "X = full_dataset[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_num = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X[:trainset_num]\n",
    "X_test = X[trainset_num:]\n",
    "\n",
    "Y_train = Y[:trainset_num]\n",
    "Y_test = Y[trainset_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulding LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 70 200\n"
     ]
    }
   ],
   "source": [
    "embeding_to_size = 200\n",
    "classes_num = 3\n",
    "print(vocab_size, max_length, embeding_to_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, embeding_to_size, input_length=max_length))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(LSTM(5, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(classes_num, activation='softmax', kernel_constraint=maxnorm(2.)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 70, 200)           400000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 5)                 4120      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 404,138\n",
      "Trainable params: 404,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb = TensorBoard('logs/tb.log', histogram_freq=1, write_grads=True)\n",
    "es = EarlyStopping(patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15528 samples, validate on 3883 samples\n",
      "Epoch 1/5\n",
      "15528/15528 [==============================] - 71s - loss: 0.9603 - acc: 0.5290 - val_loss: 0.8845 - val_acc: 0.5437\n",
      "Epoch 2/5\n",
      "15528/15528 [==============================] - 78s - loss: 0.7885 - acc: 0.6469 - val_loss: 0.7287 - val_acc: 0.6987\n",
      "Epoch 3/5\n",
      "15528/15528 [==============================] - 79s - loss: 0.6069 - acc: 0.7521 - val_loss: 0.6619 - val_acc: 0.7337\n",
      "Epoch 4/5\n",
      "15528/15528 [==============================] - 86s - loss: 0.5258 - acc: 0.7897 - val_loss: 0.6447 - val_acc: 0.7394\n",
      "Epoch 5/5\n",
      "15528/15528 [==============================] - 86s - loss: 0.4799 - acc: 0.8113 - val_loss: 0.6659 - val_acc: 0.7407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa6713f58d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=32, callbacks=[tb, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('last_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15528 samples, validate on 3883 samples\n",
      "Epoch 1/2\n",
      "15528/15528 [==============================] - 20s - loss: 0.4797 - acc: 0.8065 - val_loss: 0.7912 - val_acc: 0.7013\n",
      "Epoch 2/2\n",
      "15528/15528 [==============================] - 23s - loss: 0.4530 - acc: 0.8208 - val_loss: 0.7795 - val_acc: 0.7250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa672ec5860>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('m_77_72_r5_wodo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15528 samples, validate on 3883 samples\n",
      "Epoch 1/2\n",
      "15528/15528 [==============================] - 18s - loss: 0.4262 - acc: 0.8306 - val_loss: 0.8286 - val_acc: 0.7172\n",
      "Epoch 2/2\n",
      "15528/15528 [==============================] - 21s - loss: 0.4120 - acc: 0.8372 - val_loss: 0.8350 - val_acc: 0.7188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa672d46780>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
